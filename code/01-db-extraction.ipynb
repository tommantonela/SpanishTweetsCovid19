{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your MongoDB URI (localhost or remote)\n",
    "client = MongoClient('mongodb://localhost:27017/')  # Change the URI if needed\n",
    "\n",
    "# Access the database (replace with your actual database name)\n",
    "db = client['SocialCovid19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "first_date = datetime(2020,1,1)\n",
    "last_date = datetime(2021,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_outs = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dir_outs + '__already_processed_filtered.pickle'):\n",
    "    already_processed_files = pd.read_pickle(dir_outs + '__already_processed_filtered.pickle')\n",
    "else:\n",
    "    already_processed_files = {}\n",
    "\n",
    "added = False\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    if not file.startswith('tweets_filtered_stats'):\n",
    "        continue\n",
    "        \n",
    "    if file in already_processed_files:\n",
    "        continue\n",
    "    added = True\n",
    "    already_processed_files[file] = pd.read_pickle(dir_outs + file)['tweetId'].values\n",
    "\n",
    "if added:\n",
    "    with open(dir_outs + '__already_processed_filtered.pickle','wb') as ff:\n",
    "        pickle.dump(already_processed_files,ff)\n",
    "    \n",
    "already_processed = set()\n",
    "for v in tqdm(already_processed_files.values()):\n",
    "    already_processed.update(v)\n",
    "\n",
    "print('Already processed:',len(already_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tweets = db['tweets']\n",
    "\n",
    "projection = {'contributors': 1, 'created': 1, 'favoriteCount': 1, 'hastTags': 1, 'inReplyToScreenName': 1, 'inReplyToStatusId': 1, 'inReplyToUserId': 1,\n",
    "'lang': 1, 'mediaEntity': 1, 'quotedStatusId': 1, 'retweet': 1, 'retweetCount': 1, 'retweetId': 1, 'text': 1, 'tweetId': 1, 'urlEntities': 1,\n",
    "'userId': 1, 'userMentions': 1, 'place':1}\n",
    "\n",
    "tweets_stats = deque()\n",
    "tweets_per_day = defaultdict(deque)\n",
    "\n",
    "chunk_size = 10_000\n",
    "for ff in tqdm(os.listdir(dir_outs)):\n",
    "    if not ff.startswith('_tweets_to_extract'): # it's simply a pickle file with a dictionary of tweets {date : set(tweet_id)}, date is of the format 'YYYY-M', we built it using the 01_Tweets_user_createdAt_place_ we already had the db created and whant to extract some tweets from it, assuming we had downloaded a larger tweet set\n",
    "        continue\n",
    "        \n",
    "    tt = pd.read_pickle(dir_outs + ff)\n",
    "    \n",
    "    for month,tweets in tt.items():\n",
    "        tweets = [x for x in tweets if x not in already_processed]\n",
    "                \n",
    "        for i in range(0, len(tweets), chunk_size):\n",
    "            ids = tweets[i:i + chunk_size]\n",
    "            \n",
    "            with client.start_session() as session:\n",
    "                try: \n",
    "\n",
    "                    cursor = db_tweets.find({'tweetId':{'$in':ids}}, projection=projection, session=session, no_cursor_timeout=True)\n",
    "\n",
    "                    for tt in tqdm(cursor):\n",
    "                       \n",
    "                        if tt['tweetId'] in already_processed: # should not be needed\n",
    "                            continue\n",
    "\n",
    "                        if tt['lang'] != 'es':\n",
    "                            continue\n",
    "\n",
    "                        if tt['created'] < first_date or (last_date is not None and tt['created'] > last_date):\n",
    "                            continue\n",
    "\n",
    "                        tweets_stats.append({\n",
    "                            'tweetId': tt['tweetId'],\n",
    "                            'userId': tt['userId'],\n",
    "                            'created': tt['created'],\n",
    "                            'contributors': tt['contributors'],\n",
    "                            'favoriteCount': tt['favoriteCount'],\n",
    "                            'hastTags': tt['hastTags'],\n",
    "                            'inReplyToScreenName': tt.get('inReplyToScreenName'),\n",
    "                            'inReplyToStatusId': tt['inReplyToStatusId'],\n",
    "                            'inReplyToUserId': tt['inReplyToUserId'],\n",
    "                            'mediaEntity': tt['mediaEntity'],\n",
    "                            'quotedStatusId': tt.get('quotedStatusId',-1),\n",
    "                            'retweet': tt['retweet'],\n",
    "                            'retweetCount': tt['retweetCount'],\n",
    "                            'retweetId': tt.get('retweetId',-1),\n",
    "                            'urlEntities': tt['urlEntities'],  \n",
    "                            'userMentions':tt['userMentions'],\n",
    "                            'placeId': tt.get('place')})\n",
    "\n",
    "                        if tt['retweet'] or tt.get('retweetId',-1) != -1: # we don't count retweets here\n",
    "                            continue\n",
    "\n",
    "                        day = tt['created'].strftime('%Y-%m-%d')\n",
    "                        tweets_per_day[day].append({'tweetId':tt['tweetId'],'userId':tt['userId'],'text':tt['text'].replace('\\n',' ')})\n",
    "\n",
    "                        # depending on the number of tweets in the cursor, an additional save step could be added here\n",
    "\n",
    "                    for day in tweets_per_day:\n",
    "                        if len(tweets_per_day[day]) > 0:\n",
    "                            print(dir_outs + f'day_{day}_{str(int(datetime.now().timestamp()))}.pickle')\n",
    "                            pd.DataFrame(tweets_per_day[day]).to_pickle(dir_outs + f'day_{day}_{str(int(datetime.now().timestamp()))}.pickle')\n",
    "                            tweets_per_day[day].clear()\n",
    "\n",
    "                    if len(tweets_stats) > 0:\n",
    "                            pd.DataFrame(tweets_stats).to_pickle(dir_outs + f'tweets_filtered_stats{str(int(datetime.now().timestamp()))}.pickle')\n",
    "                            tweets_stats.clear()\n",
    "\n",
    "                finally:\n",
    "                    cursor.close()  # Ensure the cursor is closed after use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the different files in Mendeley from the extracted db snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_ids(outs_dir,prefix,col='tweet_id',convert=True):\n",
    "    processed = set()\n",
    "    for ff in os.listdir(outs_dir):\n",
    "        if not ff.startswith(prefix):\n",
    "            continue\n",
    "        processed.update(pd.read_pickle(outs_dir + ff)[col].values)\n",
    "    \n",
    "    if convert:\n",
    "        processed = set([int(x,32) for x in processed])\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "digs = string.digits + string.ascii_letters\n",
    "\n",
    "def int2base(x, base=32):\n",
    "    if x < 0:\n",
    "        sign = -1\n",
    "    elif x == 0:\n",
    "        return digs[0]\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    x *= sign\n",
    "    digits = []\n",
    "\n",
    "    while x:\n",
    "        digits.append(digs[x % base])\n",
    "        x = x // base\n",
    "\n",
    "    if sign < 0:\n",
    "        digits.append('-')\n",
    "\n",
    "    digits.reverse()\n",
    "\n",
    "    return ''.join(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = {}\n",
    "db_places = db['places']  \n",
    "with client.start_session() as session:\n",
    "    try: # projection = {'name': 1, 'age': 1}\n",
    "        cursor = db_places.find({}, projection={'fullName':1, 'country':1, 'placeId':1}, session=session, no_cursor_timeout=True)\n",
    "\n",
    "        for pp in tqdm(cursor):\n",
    "            places[pp['placeId']] = pp\n",
    "\n",
    "    finally:\n",
    "        cursor.close()  # Ensure the cursor is closed after use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'tweets_filtered_stats_' #'tweets_filtered_stats' #all_tweets_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 .Tweets_user_createdAt_place\n",
    "# tweet_id: the id of each Twitter post in Long format.\n",
    "# user_id: the id of the user that shared the post\n",
    "# created_at: date and time of posting in Long format.\n",
    "# place_fullName: full name of the included place if available.\n",
    "# place_country: country of the included place if available.\n",
    "\n",
    "table_01 = pd.DataFrame()\n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'01_Tweets_user_createdAt_place_','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(file_prefix):\n",
    "        continue\n",
    "    \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)[['tweetId','userId','created','placeId']]\n",
    "    \n",
    "    df_tweets['place_fullName'] = [None if x is None else places[x]['fullName'] for x in df_tweets['placeId']]\n",
    "    df_tweets['place_country'] = [None if x is None else places[x]['fullName'] for x in df_tweets['country']]\n",
    "\n",
    "    df_tweets['tweetId'] = [int2base(x,32) for x in df_tweets['tweetId']]\n",
    "    df_tweets['userId'] = [int2base(x,32) for x in df_tweets['userId']]\n",
    "    df_tweets['created'] = [int2base(int(x.astype('M8[s]').tolist().timestamp()),,32) for x in df_tweets['created']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id','userId':'user_id','created':'created_at'})\n",
    "\n",
    "    table_01 = pd.concat([table_01,df_tweets])\n",
    "    \n",
    "    if len(table_01) >= 15_000_000: # saving in chunks to avoid losing computations due to unforeen events. Stored as parquet to save space\n",
    "        table_01.to_parquet(dir_outs + f'01_Tweets_user_createdAt_place_{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        tabl_01 = pd.DataFrame()\n",
    "        \n",
    "if len(table_01) > 0:\n",
    "    table_01.to_parquet(dir_outs + f'01_Tweets_user_createdAt_place_{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    tabl_01 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Tweets_type\n",
    "# tweet_id.\n",
    "# original: 1 if the tweet is an original post, 0 otherwise.\n",
    "# retweet: the retweeted tweet_id if a retweet, 0 otherwise.\n",
    "# reply: the replied tweet id, 0 otherwise.\n",
    "# quote: the quoted tweet_id, 0 otherwise. This can only be combined with Original or Reply.\n",
    "\n",
    "table_02 = pd.DataFrame()\n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'02_Tweets_type','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(file_prefix):\n",
    "        continue\n",
    "    \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)[['tweetId','retweetId','quotedStatusId','inReplyToStatusId']]\n",
    "\n",
    "    if any(x in already_processed for x in df_tweets['tweetId'].values):\n",
    "        continue\n",
    "\n",
    "    df_tweets['retweetId'] = df_tweets['retweetId'].replace(-1,0)\n",
    "    df_tweets['inReplyToStatusId'] = df_tweets['inReplyToStatusId'].replace(-1,0)\n",
    "    df_tweets['quotedStatusId'] = df_tweets['quotedStatusId'].replace(-1,0)\n",
    "\n",
    "    df_tweets['original'] = [1 if x == 0 else 0 for x in df_tweets['retweetId']]\n",
    "\n",
    "    df_tweets['tweetId'] = [int2base(x) for x in df_tweets['tweetId']]\n",
    "    df_tweets['retweetId'] = [int2base(x) for x in df_tweets['retweetId']]\n",
    "    df_tweets['inReplyToStatusId'] = [int2base(x) for x in df_tweets['inReplyToStatusId']]\n",
    "    df_tweets['quotedStatusId'] = [int2base(x) for x in df_tweets['quotedStatusId']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id','retweetId':'retweet','quotedStatusId':'quote','inReplyToStatusId':'reply'})\n",
    "\n",
    "    df_tweets = df_tweets[['tweet_id','original','retweet','reply','quote']]\n",
    "\n",
    "    table_02 = pd.concat([table_02,df_tweets])\n",
    "\n",
    "    if len(table_02) >= 15_000_000:\n",
    "        table_02.to_parquet(dir_outs + f'02_Tweets_type{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_02 = pd.DataFrame()\n",
    "        \n",
    "if len(table_02) > 0:\n",
    "    table_02.to_parquet(dir_outs + f'02_Tweets_type{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_02 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Tweets_media_url_contributors_mentions\n",
    "# tweet_id.\n",
    "# media: number of media elements in the tweet.\n",
    "# url: number of urls in the tweet.\n",
    "# contributors: number of contributors in the tweet.\n",
    "# mentions: number of mentions of other users in the tweet.\n",
    "\n",
    "table_03 = pd.DataFrame()\n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'03_Tweets_media_url_contributors_mentions','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(file_prefix'):\n",
    "        continue\n",
    "    \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)[['tweetId','mediaEntity','urlEntities','contributors','userMentions']]\n",
    "    \n",
    "    if any(x in already_processed for x in df_tweets['tweetId'].values):\n",
    "        continue\n",
    "\n",
    "    df_tweets['mediaEntity'] = [0 if x is None else len(x) for x in df_tweets['mediaEntity']]\n",
    "    df_tweets['urlEntities'] = [0 if x is None else len(x) for x in df_tweets['urlEntities']]\n",
    "    df_tweets['contributors'] = [0 if x is None else len(x) for x in df_tweets['contributors']]\n",
    "    df_tweets['userMentions'] = [0 if x is None else len(x) for x in df_tweets['userMentions']]\n",
    "\n",
    "    df_tweets['tweetId'] = [int2base(x) for x in df_tweets['tweetId']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id',\n",
    "                                          'mediaEntity':'media','urlEntities':'url','userMentions':'mentions'})\n",
    "\n",
    "    table_03 = pd.concat([table_03,df_tweets])\n",
    "\n",
    "    if len(table_03) >= 15_000_000:\n",
    "        table_03.to_parquet(dir_outs + f'03_Tweets_media_url_contributors_mentions{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_03 = pd.DataFrame()\n",
    "        \n",
    "if len(table_03) > 0:\n",
    "    table_03.to_parquet(dir_outs + f'03_Tweets_media_url_contributors_mentions{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_03 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Tweets_hashtags\n",
    "# tweet_id.\n",
    "# hashtags: a list with the hashtags included in the tweet.\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile('#[a-zA-ZáéíóúÁÉÍÓÚüÜñÑ0-9_]+(?=\\s|[^\\w\\s]|$)')\n",
    "\n",
    "table_04 = pd.DataFrame()\n",
    "\n",
    "prefix = \"day_\"\n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'04_Tweets_hashtags','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(prefix):\n",
    "        continue\n",
    "    \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)\n",
    "\n",
    "    if len(df_tweets) == 0:\n",
    "        continue\n",
    "\n",
    "    if any(x in already_processed for x in df_tweets['tweetId'].values):\n",
    "        continue\n",
    "\n",
    "    df_tweets['hashtags'] = ['\\t'.join(hashtag_re.findall(x)) for x in df_tweets['text'].values]\n",
    "    df_tweets = df_tweets[df_tweets['hashtags'].map(len) > 0]\n",
    "    \n",
    "    df_tweets['tweetId'] = [int2base(x) for x in df_tweets['tweetId']]\n",
    "\n",
    "    df_tweets = df_tweets[['tweetId','hashtags']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id'})\n",
    "\n",
    "    table_04 = pd.concat([table_04,df_tweets])\n",
    "\n",
    "    if len(table_04) >= 1_000_000:\n",
    "        table_04.to_parquet(dir_outs + f'04_Tweets_hashtags{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_04 = pd.DataFrame()\n",
    "\n",
    "if len(table_04) > 0:\n",
    "    table_04.to_parquet(dir_outs + f'04_Tweets_hashtags{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_04 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Tweets_urls\n",
    "# tweet_id.\n",
    "# urls: a tab separated list with the urls included in the tweet.\n",
    "\n",
    "table_05 = pd.DataFrame()\n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'05_Tweets_urls','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(file_prefix):\n",
    "        continue\n",
    "    \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)[['tweetId','urlEntities']]\n",
    "\n",
    "    if any(x in already_processed for x in df_tweets['tweetId'].values):\n",
    "        continue\n",
    "\n",
    "    df_tweets = df_tweets[df_tweets['urlEntities'].map(len) > 0]\n",
    "    \n",
    "    df_tweets['urlEntities'] = ['' if len(x) == 0 else '\\t'.join([y['expandedURL'] for y in x]) for x in df_tweets['urlEntities']]\n",
    "\n",
    "    df_tweets['tweetId'] = [int2base(x) for x in df_tweets['tweetId']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id',\n",
    "                                          'urlEntities':'urls'})\n",
    "\n",
    "    table_05 = pd.concat([table_05,df_tweets])\n",
    "    \n",
    "    if len(table_05) >= 15_000_000:\n",
    "        table_05.to_parquet(dir_outs + f'05_Tweets_urls{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_05 = pd.DataFrame()\n",
    "\n",
    "if len(table_05) > 0:\n",
    "    table_05.to_parquet(dir_outs + f'05_Tweets_urls{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_05 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Tweets_mentions\n",
    "# tweet_id.\n",
    "# mentions: a tab separated list with the user_id mentions included in the tweet.\n",
    "\n",
    "table_06 = pd.DataFrame() \n",
    "\n",
    "already_processed = get_processed_ids(dir_outs,'06_Tweets_mentions','tweet_id')\n",
    "print('Already processed:',len(already_processed))\n",
    "\n",
    "for file in tqdm(os.listdir(dir_outs)):\n",
    "    \n",
    "    if not file.startswith(file_prefix):\n",
    "        continue\n",
    "        \n",
    "    df_tweets = pd.read_pickle(dir_outs + file)[['tweetId','userMentions']]\n",
    "\n",
    "    if any(x in already_processed for x in df_tweets['tweetId'].values):\n",
    "        continue\n",
    "\n",
    "    df_tweets = df_tweets[df_tweets['userMentions'].map(len) > 0]\n",
    "    \n",
    "    df_tweets['userMentions'] = ['' if len(x) == 0 else '\\t'.join([int2base(y['userId']) for y in x]) for x in df_tweets['userMentions']]\n",
    "\n",
    "    df_tweets['tweetId'] = [int2base(x) for x in df_tweets['tweetId']]\n",
    "\n",
    "    df_tweets = df_tweets.rename(columns={'tweetId':'tweet_id',\n",
    "                                          'userMentions':'mentions'})\n",
    "\n",
    "    table_06 = pd.concat([table_06,df_tweets])\n",
    "\n",
    "    if len(table_06) >= 15_000_000:\n",
    "        table_06.to_parquet(dir_outs + f'06_Tweets_mentions{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_06 = pd.DataFrame()\n",
    "\n",
    "if len(table_06) > 0:\n",
    "    table_06.to_parquet(dir_outs + f'06_Tweets_mentions{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_06 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Tweets_replies\n",
    "# tweet_id\n",
    "# replies_ids: a tab separated list with the tweet_id of replies.\n",
    "\n",
    "table_07 = deque() \n",
    "\n",
    "with client.start_session() as session:\n",
    "    try: # projection = {'name': 1, 'age': 1}\n",
    "        cursor = db['tweetReplies'].find({}, projection={'tweetId':1,'replies':1}, session=session, no_cursor_timeout=True)\n",
    "\n",
    "        for tt in tqdm(cursor):\n",
    "            if len(tt['replies']) > 0:\n",
    "                rr = [int2base(x) for x in tt['replies'] if x != -1]\n",
    "                if len(rr) > 0:\n",
    "                    table_07.append({'tweet_id':tt['tweetId'],'replies': '\\t'.join(rr)})\n",
    "               \n",
    "    finally:\n",
    "        cursor.close()  # Ensure the cursor is closed after use\n",
    "        \n",
    "table_07 = pd.DataFrame(table_07)\n",
    "table_07.to_csv(dir_outs + '07_Tweets_replies.csv',index=False)\n",
    "table_07.to_parquet(dir_outs + '07_Tweets_replies.parquet',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Users\n",
    "# user_id.\n",
    "# location: the user defined location for the account. It might not be a real location nor machine parseable.\n",
    "# created_at: date and time of account creation in Long format.\n",
    "# hasProfileImage: 1 if the user customized the profile account, 0 if the user kept the default profile image.\n",
    "# follower_count: number of followers of the user.\n",
    "# followee_count: number of followees of the user. \n",
    "# statuses_count: number of published tweets, regardless of their type.\n",
    "# is_verified: whether the account is verified.\n",
    "# favorites_count: the number of tweets this user has liked in the account’s lifetime.\n",
    "# listed_count: the number of public lists that this user is a member of.\n",
    "\n",
    "# Note: this retrieves all users in the db. It could be restricted to get only the users in the file 01_Tweets_user_createdAt_place_\n",
    "\n",
    "table_08 = deque() \n",
    "\n",
    "projection = {\"defaultProfileImage\" : 1, \"created\": 1, \"listedCount\" : 1,  \"location\" : 1,  \"favoritesCount\" : 1,  \n",
    "              \"followerCount\" : 1, \"friendsCount\" : 1,  \"statusesCount\" : 1, \"userId\" : 1,  \"verified\" : 1 }\n",
    "\n",
    "with client.start_session() as session:\n",
    "    try: \n",
    "        cursor = db['users'].find({}, projection=projection, session=session, no_cursor_timeout=True)\n",
    "\n",
    "        for uu in tqdm(cursor):\n",
    "            table_08.append({'user_id':uu['tweetId'],\n",
    "                             'location':uu['location'], 'created_at':uu['created'], 'hasProfileImage':uu['defaultProfileImage'], 'follower_count':uu[''],\n",
    "                             'followee_count':uu['friendsCount'], 'statuses_count':uu['statusesCount'], 'is_verified':uu['verified'], 'favorites_count':uu['favoritesCount'], 'listed_count':uu['listedCount']})\n",
    "               \n",
    "    finally:\n",
    "        cursor.close()  # Ensure the cursor is closed after use\n",
    "\n",
    "    if len(table_08) >= 15_000_000:\n",
    "        pd.DataFrame(table_08).to_parquet(dir_outs + f'08_Users{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "        table_08.clear()\n",
    "\n",
    "if len(table_08) > 0:\n",
    "    pd.DataFrame(table_08).to_parquet(dir_outs + f'08_Users{str(int(datetime.now().timestamp()))}.parquet',index=False)\n",
    "    table_08.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09-User_graph.csv\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "user_tweets = Counter()\n",
    "tweet_user_map = {}\n",
    "\n",
    "df_1 = pd.read_csv(dir_outs + '01-Tweets_user_createdAt_place.csv')\n",
    "\n",
    "for i in tqdm(range(0,len(df_1))):\n",
    "    user_tweets[df_1['user_id'].values[i]] += 1\n",
    "    tweet_user_map[df_1['tweet_id'].values[i]] = df_1['user_id'].values[i]\n",
    "\n",
    "del df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(dir_outs + '02-Tweets_type.csv')\n",
    "\n",
    "user_relations = defaultdict(defaultdict(set).copy)\n",
    "for i in tqdm(range(0,len(df_2))):\n",
    "\n",
    "    if df_2['tweet_id'].values[i] not in tweet_user_map:\n",
    "        continue\n",
    "\n",
    "    if df_2['retweet'].values[i] in tweet_user_map:\n",
    "        user_relations[tweet_user_map[df_2['tweet_id'].values[i]]]['retweet'].add(tweet_user_map[df_2['retweet'].values[i]])\n",
    "    \n",
    "    if df_2['reply'].values[i] in tweet_user_map:\n",
    "        user_relations[tweet_user_map[df_2['tweet_id'].values[i]]]['reply'].add(tweet_user_map[df_2['reply'].values[i]])\n",
    "\n",
    "    if df_2['quote'].values[i] in tweet_user_map:\n",
    "        user_relations[tweet_user_map[df_2['tweet_id'].values[i]]]['quote'].add(tweet_user_map[df_2['quote'].values[i]])\n",
    "\n",
    "del df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = pd.read_csv(dir_outs + '07-Tweets_replies.csv')\n",
    "for i in tqdm(range(0,len(df_7))):\n",
    "\n",
    "    if df_7['tweet_id'].values[i] not in tweet_user_map:\n",
    "        continue\n",
    "\n",
    "    rr = df_7['replies'].values[i].split('\\t')\n",
    "    for rep in rr:\n",
    "        if rep in tweet_user_map:\n",
    "            user_relations[tweet_user_map[rep]]['reply'].add(tweet_user_map[df_7['tweet_id'].values[i]])\n",
    "\n",
    "del df_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = pd.read_csv(dir_outs + '06-Tweets_mentions.csv')\n",
    "\n",
    "for i in tqdm(range(0,len(df_6))):\n",
    "\n",
    "    if df_6['tweet_id'].values[i] not in tweet_user_map:\n",
    "        continue\n",
    "\n",
    "    rr = df_6['mentions'].values[i].split('\\t')\n",
    "    for rep in rr:\n",
    "        user_relations[tweet_user_map[df_6['tweet_id'].values[i]]]['mention'].add(rep)\n",
    "\n",
    "del df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_names = {1:\"retweet\",2:\"reply\",3:\"quote\",4:\"mention\"}\n",
    "inverse_map = {v:k for k,v in map_names.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "ll = deque()\n",
    "for user,rels in tqdm(user_relations.items()):\n",
    "\n",
    "    for rt in rels:\n",
    "\n",
    "        ll.append({'user_id':user,'relations':inverse_map[rt],'ids': '\\t'.join(rels[rt])})\n",
    "\n",
    "df_9 = pd.DataFrame(ll)\n",
    "\n",
    "df_9.to_csv(dir_outs + '09-Users_graph.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
