{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Empathm(Empath): # we extend the original Empath code to be able to load our own category file\n",
    "    def __init__(self, categories_file=\"./data/categories.tsv\",user_dir=\"./data/user/\"):\n",
    "        self.cats = defaultdict(list)\n",
    "        self.inv_cache = {}\n",
    "        self.load(categories_file)\n",
    "        for f in os.listdir(user_dir):\n",
    "            if len(f.split(\".\")) > 1 and f.split(\".\")[1] == \"empath\":\n",
    "                self.load(user_dir+f)\n",
    "\n",
    "        self.categories = self.cats.keys()\n",
    "        self.invcats = defaultdict(list)\n",
    "        key = tuple(sorted(self.categories))\n",
    "        if key in self.inv_cache:\n",
    "            self.invcats = self.inv_cache[key]\n",
    "        else:\n",
    "            for k in self.categories:\n",
    "                for t in self.cats[k]: \n",
    "                    self.invcats[t].append(k)\n",
    "            self.inv_cache[key] = self.invcats\n",
    "                                \n",
    "    def load(self,file):\n",
    "        with open(file,\"r\") as f:\n",
    "            for line in f:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                name = cols[0]\n",
    "                terms = cols[1:]\n",
    "                for t in set(terms):\n",
    "                    self.cats[name].append(t)\n",
    "                    \n",
    "                                 \n",
    "    def analyzePrefix(self,doc,categories=None,tokenizer=\"default\",normalize=False):  # we extend the original analyze method to consider prefixes instead of full matchings\n",
    "\n",
    "        count = {}\n",
    "        tokens = 0.0\n",
    "        for cat in self.categories: \n",
    "            count[cat] = 0.0\n",
    "        for tk in doc.split():\n",
    "            tokens += 1.0\n",
    "            for cat in self.invcats[tk]:\n",
    "                count[cat]+=1.0\n",
    "        if normalize:\n",
    "            for cat in count.keys():\n",
    "                if tokens == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    count[cat] = count[cat] / tokens\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emphat(root,file,lexicon,name):\n",
    "    \n",
    "    if os.path.exists(root+\"/empath_\"+name+file):\n",
    "        return\n",
    "    \n",
    "    if file.endswith('csv'):\n",
    "        df = pd.read_csv(root+\"/\"+file).dropna()\n",
    "    else:\n",
    "        df = pd.read_pickle(root+'/'+file).dropna()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    \n",
    "    print(\"Processing\",file)\n",
    "    \n",
    "    listi = []\n",
    "    for index, row in tqdm(df.iterrows()): \n",
    "        d = lexicon.analyzePrefix(row['text'], normalize=True)\n",
    "        d =  {key:val for key, val in d.items() if val > 0}\n",
    "        if len(d) > 0: # no need to store empty texts\n",
    "            d['tweetId'] = row['tweetId']\n",
    "            listi.append(d)\n",
    "\n",
    "    cats = pd.DataFrame(listi)\n",
    "    cats = cats.reindex(sorted(cats.columns), axis=1)\n",
    "    if 'tweetId' in cats:\n",
    "        col = cats.pop(\"tweetId\")\n",
    "        cats.insert(0, col.name, col) # in place\n",
    "    cats.to_csv(root+\"/empath_\"+name+file,index=False,encoding='utf8')\n",
    "\n",
    "def loop_process_emphat(datapath,prefix,user_path='./data/users/',categories_file=\"./data/es_categories.tsv\",name=''):\n",
    "    \n",
    "    lexicon = Empathm(user_dir=user_path,categories_file=categories_file)\n",
    "    \n",
    "    path = os.walk(datapath)\n",
    "    for root, _, files in path:\n",
    "        for file in tqdm(files):\n",
    "            if file.startswith(prefix):\n",
    "                process_emphat(root,file,lexicon,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')  # Open Multilingual WordNet\n",
    "nltk.download('punkt') # tokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "sentiSensePath = dir_root + '/data/SentiSense_Synsets_ES_30.xml' # not a resource created as part of this projects, need to be downloaded\n",
    "tree = ET.parse(sentiSensePath)\n",
    "root = tree.getroot()\n",
    "\n",
    "sentiSenseDict={}\n",
    "\n",
    "for child in root:\n",
    "    key = child.attrib.get('synset')[4:]\n",
    "    key = key[:-2]\n",
    "    sentiSenseDict[key] = child.attrib.get('emotion')\n",
    "\n",
    "import csv\n",
    "\n",
    "wordnetLAS = dir_root + '/data/wn-data-las.csv'\n",
    "reader = csv.DictReader(open(wordnetLAS,encoding='utf8'))\n",
    "\n",
    "wordnetLASDict = {}\n",
    "\n",
    "for row in reader:\n",
    "    if row.get('SYNSET_OFFSET') in sentiSenseDict:\n",
    "        value = row.get('SYNSET_OFFSET')+\"-\"+row.get('POS_TAG')\n",
    "        wordnetLASDict[row.get('LEMMA')] = value.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_wordnet(root,file,column='text'): \n",
    "    \n",
    "    list_emotions = []\n",
    "    if file.endswith('csv'):\n",
    "        df = pd.read_csv(root+\"/\"+file).dropna()\n",
    "    else:\n",
    "        df = pd.read_pickle(root + '/' + file).dropna()\n",
    "        \n",
    "    if len(df) == 0:\n",
    "        return\n",
    "       \n",
    "    print('Processing '+file)\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows()): \n",
    "        freqs = Counter()\n",
    "        \n",
    "        if len(row[column].strip()) > 1:\n",
    "            \n",
    "            try:\n",
    "                sent_tokens = word_tokenize(row[column]) \n",
    "\n",
    "                for word in sent_tokens: \n",
    "                    wordnet_word = wordnetLASDict.get(word.lower())\n",
    "                    if wordnet_word is not None:\n",
    "                        emotion = sentiSenseDict.get(wordnet_word[0:-2])\n",
    "                        if emotion is not None:\n",
    "                            freqs[emotion] += 1\n",
    "            except IndexError:\n",
    "                freqs = {'tweetId':row['tweetId']}\n",
    "        else:\n",
    "                freqs = {'tweetId':row['tweetId']}\n",
    "        \n",
    "        freqs['tweetId'] = row['tweetId']\n",
    "        list_emotions.append(freqs)\n",
    "    \n",
    "    df = pd.DataFrame(list_emotions)\n",
    "    df = df.set_index('tweetId')\n",
    "    df = df.div(df.sum(axis=1), axis=0) # relative weight in row\n",
    "    \n",
    "    df.to_csv(root+'/'+'sentiwordnet_'+file)\n",
    "\n",
    "def loop_senti_wordnet(datapath,prefix,column='text'):\n",
    "    path = os.walk(datapath)\n",
    "    for root, _, files in path:\n",
    "        for file in tqdm(files):\n",
    "            if not file.startswith(prefix) or os.path.exists(root+'/'+'sentiwordnet_'+file):\n",
    "                continue\n",
    "            \n",
    "            senti_wordnet(root,file,column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"./\" \n",
    "prefix = \"day_\"\n",
    "dir_outs = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_file = dir_root + \"/data/es_categories.tsv\"\n",
    "\n",
    "loop_process_emphat(datapath,prefix,categories_file=categories_file)\n",
    "\n",
    "categories_file = dir_root + '/data/es_categories_mental_health.tsv'\n",
    "name = 'mental_'\n",
    "loop_process_emphat(datapath,prefix,categories_file=categories_file,name=name)\n",
    "\n",
    "categories_file = dir_root + '/data/es_categories_crisis.tsv'\n",
    "name = 'crisis_'\n",
    "loop_process_emphat(datapath,prefix,categories_file=categories_file,name=name)\n",
    "\n",
    "loop_senti_wordnet(datapath,prefix,column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on the number of files or tweets to process, a unification of day files might be needed -- this loops over all created day_ files\n",
    "file_names = defaultdict(set)\n",
    "\n",
    "already_computed = defaultdict(set) if not os.path.exists(dir_outs + 'already_processed_files.pickle') else pd.read_pickle(dir_outs + 'already_processed_files.pickle')\n",
    "\n",
    "for ff in tqdm(os.listdir(dir_outs)):\n",
    "        \n",
    "    if ff.endswith('.zip') or ff.startswith('processed'):\n",
    "        continue\n",
    "    \n",
    "    if ff.startswith('day') or ff.startswith('_') or ff.startswith('tweets_filtered') or ff.startswith('__alr') or ff.startswith('already'):\n",
    "        continue\n",
    "                        \n",
    "    if '_'.join(ff.split('_')[0:-1]) not in already_computed or not ff in already_computed['_'.join(ff.split('_')[0:-1])]:\n",
    "        file_names['_'.join(ff.split('_')[0:-1])].add(ff)\n",
    "\n",
    "for key,files in file_names.items():\n",
    "    print(key,len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for key,files in tqdm(file_names.items()):\n",
    "    print(key)\n",
    "    dd = pd.DataFrame()\n",
    "    for ff in tqdm(files):\n",
    "        try:\n",
    "            aa = pd.read_csv(dir_outs + ff)\n",
    "            dd = pd.concat([dd,aa])\n",
    "        except pd.errors.EmptyDataError:\n",
    "            pass\n",
    "    dd = dd.drop_duplicates(subset=['tweetId'])\n",
    "    \n",
    "    if not os.path.exists(dir_outs + 'processed_files/' + key + '.csv'):\n",
    "        dd.to_csv(dir_outs + 'processed_files/' + key + '.csv')\n",
    "    else:\n",
    "        dd = pd.concat([dd,pd.read_csv(dir_outs + 'processed_files/' + key + '.csv')])\n",
    "        dd = dd.drop_duplicates(subset=['tweetId'])\n",
    "        dd.to_csv(dir_outs + 'processed_files/' + key + '.csv')\n",
    "    \n",
    "    # here we update the already_processed_files, so in next evaluation there's no need to reprocess this\n",
    "    already_computed[key].update(files)\n",
    "    with open(dir_outs + 'already_processed_files.pickle','wb') as fil:\n",
    "        pickle.dump(already_computed,fil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_representation(df,column,which='median',file=None):\n",
    "        \n",
    "    if which == 'median':\n",
    "        return df[column].median()\n",
    "    \n",
    "    if which == 'mean':\n",
    "        return df[column].mean()\n",
    "\n",
    "    if which == 'per_tweets': \n",
    "        if len(df[column]) == 0: \n",
    "            return 0\n",
    "        else:\n",
    "            return len(df[df[column] > 0]) / len(df[column])\n",
    "\n",
    "    df_aux = pd.read_csv(file)\n",
    "\n",
    "    df_aux = pd.concat([pd.DataFrame(df[column]).reset_index(),df_aux],axis=1)\n",
    "    \n",
    "    return len(set(~df_aux[df_aux[column].isna()]['userId'])) / len(set(df_aux['userId']))\n",
    "\n",
    "def load_categories(datapath,prefix,which='median',selected_cats=None,summarized=True): \n",
    "\n",
    "    path = os.walk(datapath)\n",
    "\n",
    "    values = {}\n",
    "    cats = set() \n",
    "\n",
    "    for root, _, files in path:\n",
    "        for file in tqdm(files):\n",
    "\n",
    "            if not file.startswith(prefix):\n",
    "                continue\n",
    "                \n",
    "            if file.endswith('.zip'):\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(root+'/'+file)\n",
    "            df = df.set_index('tweetId')\n",
    "            cats.update(df.columns)\n",
    "            \n",
    "            dicti = {}\n",
    "            for c in df.columns:\n",
    "                if selected_cats is None or c in selected_cats: \n",
    "                  \n",
    "                    if summarized:                    \n",
    "                        dicti[c] = get_representation(df,column=c,which=which,file=root+'/'+'day'+file.replace(prefix,''))\n",
    "                    else: \n",
    "                        dicti[c] = df[c].dropna().values \n",
    "                   \n",
    "            values[file] = dicti\n",
    "    \n",
    "    dates = list(values.keys())\n",
    "    dates.sort()\n",
    "    \n",
    "    if selected_cats is not None:\n",
    "        return values,dates,selected_cats\n",
    "    \n",
    "    return values,dates,cats\n",
    "\n",
    "def transform_df(values,dates,cat): \n",
    "    ee = pd.DataFrame()\n",
    "    for d in dates:\n",
    "        vc = values[d].get(cat)\n",
    "        if vc is not None:\n",
    "            ee = pd.concat([ee,pd.DataFrame(vc,columns=[d])], axis=1,copy=False)\n",
    "    return ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'empath_day'\n",
    "\n",
    "cati = None\n",
    "valstD, datestD, catstD = load_categories(datapath,prefix,which='per_tweets',selected_cats=cati,summarized=True)\n",
    "\n",
    "dataftD = pd.DataFrame(valstD,columns=datestD) # matrix\n",
    "dataftD.to_csv(f\"{datapath}/all_{prefix}_full.csv\")\n",
    "dataftD\n",
    "\n",
    "prefix = 'sentiwordnet_day'\n",
    "cati = None\n",
    "valstD, datestD, catstD = load_categories(datapath,prefix,which='per_tweets',selected_cats=cati,summarized=True)\n",
    "\n",
    "dataftD = pd.DataFrame(valstD,columns=datestD) # matrix\n",
    "dataftD.to_csv(f\"{datapath}/all_{prefix}_full.csv\")\n",
    "dataftD\n",
    "\n",
    "prefix = 'empath_crisis'\n",
    "cati = None\n",
    "valstD, datestD, catstD = load_categories(datapath,prefix,which='per_tweets',selected_cats=cati,summarized=True)\n",
    "\n",
    "dataftD = pd.DataFrame(valstD,columns=datestD) # matrix\n",
    "dataftD.to_csv(f\"{datapath}/all_{prefix}_full.csv\")\n",
    "dataftD\n",
    "\n",
    "prefix = 'empath_mental'\n",
    "cati = None\n",
    "valstD, datestD, catstD = load_categories(datapath,prefix,which='per_tweets',selected_cats=cati,summarized=True)\n",
    "\n",
    "dataftD = pd.DataFrame(valstD,columns=datestD) # matrix\n",
    "dataftD.to_csv(f\"{datapath}/all_{prefix}_full.csv\")\n",
    "dataftD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to adjust column names\n",
    "\n",
    "aa = pd.read_csv(f\"{datapath}/all_sentiwordnet_day_full.csv\",index_col=0).T.fillna(0)\n",
    "col_names = {}\n",
    "for x in aa.columns:\n",
    "    cc = x.replace('sentiwordnet_day_','').replace('.csv','')\n",
    "    cc = cc.split('-')\n",
    "    cc = cc[0] + '-' + (cc[1] if len(cc[1]) == 2 else '0' + cc[1]) + '-' + (cc[2] if len(cc[2]) == 2 else '0' + cc[2])\n",
    "    col_names[x] = cc\n",
    "\n",
    "\n",
    "aa = aa.rename(columns=col_names)\n",
    "aa.to_csv(f'{datapath}11.Tweets_emotions.csv')\n",
    "\n",
    "\n",
    "aa = pd.read_csv(f\"{datapath}/all_empath_day_full.csv\",index_col=0).T.fillna(0)\n",
    "col_names = {}\n",
    "for x in aa.columns:\n",
    "    cc = x.replace('empath_day','').replace('.csv','')\n",
    "    cc = cc.split('-')\n",
    "    cc = cc[0] + '-' + (cc[1] if len(cc[1]) == 2 else '0' + cc[1]) + '-' + (cc[2] if len(cc[2]) == 2 else '0' + cc[2])\n",
    "    col_names[x] = cc\n",
    "\n",
    "\n",
    "aa = aa.rename(columns=col_names)\n",
    "aa.to_csv(f'{datapath}10.Tweets_empath.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
