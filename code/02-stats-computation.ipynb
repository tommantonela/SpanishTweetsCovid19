{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter,defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './'\n",
    "\n",
    "df_1 = pd.read_csv(dir_path + '01-Tweets_user_createdAt_place.csv')\n",
    "df_2 = pd.read_csv(dir_path + '02-Tweets_type.csv')\n",
    "print('Merge 1 - start')\n",
    "df_merge = df_1.merge(df_2, on='tweet_id')\n",
    "print('Merge 1 - end')\n",
    "df_merge.to_csv(dir_path + 'merge-01-02.csv', index=False)\n",
    "print('Merge 1 - saved')\n",
    "del df_merge\n",
    "del df_2\n",
    "df_3 = pd.read_csv(dir_path + '03-Tweets_media_url_contributors_mentions.csv')\n",
    "print('Merge 2 - start')\n",
    "df_merge = df_1.merge(df_3, on='tweet_id')\n",
    "print('Merge 2 - end')\n",
    "df_merge.to_csv(dir_path + 'merge-01-03.csv', index=False)\n",
    "print('Merge 2 - saved')\n",
    "del df_3\n",
    "del df_merge\n",
    "\n",
    "df_4 = pd.read_csv(dir_path + '04-Tweets_hashtags.csv')\n",
    "print('Merge 3 - start')\n",
    "df_merge = df_1.merge(df_4, on='tweet_id')\n",
    "print('Merge 3 - end')\n",
    "df_merge.to_csv(dir_path + 'merge-01-04.csv', index=False)\n",
    "print('Merge 3 - saved')\n",
    "del df_4\n",
    "del df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 500_000\n",
    "\n",
    "with_places = pd.DataFrame()\n",
    "users = pd.DataFrame()\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(dir_path + '01-Tweets_user_createdAt_place.csv', chunksize=chunksize)):\n",
    "\n",
    "    chunk['createdAt'] = pd.to_numeric(chunk['createdAt'], errors='coerce')\n",
    "\n",
    "    chunk['createdAt'] = datetime.fromtimestamp(int(chunk['createdAt'].values[0]) / 1000).strftime('%Y-%m-%d')\n",
    "    \n",
    "\n",
    "    chunk['tweetId'] = [int(x,32) for x in chunk['tweetId']]\n",
    "    chunk['userId'] = [int(x,32) for x in chunk['userId']]\n",
    "\n",
    "    users = pd.concat([users,chunk.groupby(['userId','createdAt'])[['tweetId']].count().reset_index()])\n",
    "\n",
    "    with_places = pd.concat([with_places,chunk[~chunk['place_fullName'].isna() & chunk['place_fullName'].str.len() > 1].groupby('createdAt')[['place_fullName']].count().reset_index()])\n",
    "        \n",
    "aa = users.groupby(['userId','createdAt'])[['tweetId']].sum()\n",
    "aa.to_pickle(dir_path + '__users_day_stats.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1_000_000\n",
    "stats = defaultdict(Counter)\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(dir_path + 'merge-01-02.csv', chunksize=chunksize)):\n",
    "\n",
    "    if 'createdAt' not in chunk.columns:\n",
    "        chunk['createdAt'] = [int(x,36) for x in chunk['created_at']]\n",
    "\n",
    "    chunk['createdAt'] = pd.to_numeric(chunk['createdAt'], errors='coerce') # asumo que le agregaste la columna\n",
    "    chunk = chunk[~chunk['createdAt'].isna()].copy()\n",
    "    chunk['createdAt'] = datetime.fromtimestamp(int(chunk['createdAt'].values[0]) / 1000).strftime('%Y-%m-%d')\n",
    "    chunk['all'] = 1\n",
    "    dd = chunk.groupby('createdAt')[['original','retweet','reply','quote','all']].apply(lambda x: (x > 0).sum()).to_dict(orient='index')\n",
    "    \n",
    "    for k,v in dd.items():\n",
    "        stats[k].update(v)\n",
    "\n",
    "pd.DataFrame(stats).to_csv(dir_path + '__tweet_types_per_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1_000_000\n",
    "\n",
    "stats_count = defaultdict(Counter)\n",
    "stats_freq = defaultdict(Counter)\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(dir_path + 'merge-01-03.csv', chunksize=chunksize)):\n",
    "\n",
    "    if 'createdAt' not in chunk.columns:\n",
    "        chunk['createdAt'] = [int(x,36) for x in chunk['created_at']]\n",
    "\n",
    "    chunk['createdAt'] = pd.to_numeric(chunk['createdAt'], errors='coerce') # asumo que le agregaste la columna\n",
    "    chunk = chunk[~chunk['createdAt'].isna()].copy()\n",
    "    chunk['createdAt'] = datetime.fromtimestamp(int(chunk['createdAt'].values[0]) / 1000).strftime('%Y-%m-%d')\n",
    "    chunk['all'] = 1\n",
    "\n",
    "    dd = chunk.groupby('createdAt')[['media','url','contributors','mentions','all']].apply(lambda x: (x > 0).sum()).to_dict(orient='index') \n",
    "    for k,v in dd.items():\n",
    "        stats_count[k].update(v)\n",
    "\n",
    "    dd = chunk.groupby('createdAt')[['media','url','contributors','mentions','all']].sum().to_dict(orient='index') \n",
    "    for k,v in dd.items():\n",
    "        stats_freq[k].update(v)\n",
    "\n",
    "pd.DataFrame(stats_count).to_csv(dir_path + '__stats_content_tweets_per_dia.csv')\n",
    "pd.DataFrame(stats_freq).to_csv(dir_path + '__stats_freq_content_tweets_per_dia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1_000_000\n",
    "\n",
    "hash_freq = defaultdict(Counter)\n",
    "\n",
    "with_hash_day = defaultdict(Counter)\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(dir_path + 'merge-01-04.csv', chunksize=chunksize)):\n",
    "    \n",
    "    if 'createdAt' not in chunk.columns:\n",
    "        chunk['createdAt'] = [int(x,36) for x in chunk['created_at']]\n",
    "\n",
    "    chunk['createdAt'] = pd.to_numeric(chunk['createdAt'], errors='coerce') # asumo que le agregaste la columna\n",
    "    chunk = chunk[~chunk['createdAt'].isna()].copy()\n",
    "    chunk['createdAt'] = datetime.fromtimestamp(int(chunk['createdAt'].values[0]) / 1000).strftime('%Y-%m-%d')\n",
    "    chunk['all'] = 1\n",
    "\n",
    "    # hashtag frequency by date\n",
    "    aa = chunk[~chunk['hashtags'].isna()].groupby('createdAt')[['hashtags']].agg(lambda x: ''.join(x)).to_dict(orient='index')\n",
    "    for date,hh in aa.items():\n",
    "        hash_freq[date].update(hh['hashtags'].split(' '))\n",
    "\n",
    "    aa = chunk[~chunk['hashtags'].isna()].groupby('createdAt')[['hashtags']].count().to_dict(orient='index')\n",
    "    for date,hh in aa.items():\n",
    "        with_hash_day[date].update(hh)\n",
    "\n",
    "pd.DataFrame(hash_freq).to_csv(dir_path + '__stats_hash_freq.csv')\n",
    "pd.DataFrame(with_hash_day).to_csv(dir_path + '__stats_with_hash_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1_000_000\n",
    "\n",
    "hash_per_tweet= pd.DataFrame()\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(dir_path + 'merge-01-04.csv', chunksize=chunksize)):\n",
    "    chunk['createdAt'] = pd.to_numeric(chunk['createdAt'], errors='coerce') # asumo que le agregaste la columna\n",
    "    chunk = chunk[~chunk['createdAt'].isna()].copy()\n",
    "    chunk['createdAt'] = datetime.fromtimestamp(int(chunk['createdAt'].values[0]) / 1000).strftime('%Y-%m-%d')\n",
    "    chunk = chunk[~chunk['hashtags'].isna()]\n",
    "    chunk['hashtags'] = chunk['hashtags'].apply(lambda x: len(x.split(' ')))\n",
    "    aa = chunk.groupby('createdAt')[['hashtags']].agg(['count','sum']).reset_index()\n",
    "    hash_per_tweet = pd.concat([hash_per_tweet,aa])\n",
    "    \n",
    "\n",
    "hash_per_tweet.to_csv(dir_path + 'hash_per_tweet.csv', index=False)\n",
    "hash_per_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_information(df,col=None):\n",
    "\n",
    "    if col is None:\n",
    "        df['date'] = pd.to_datetime(df.index)\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df[col])\n",
    "        \n",
    "    df['month'] = df['date'].dt.year.astype(str) + '-' + df['date'].dt.month.astype(str)\n",
    "\n",
    "    df['week_of_year'] = df['date'].dt.year.astype(str) + '-' + df['date'].dt.isocalendar().week.astype(str)\n",
    "\n",
    "    df = df.drop(columns=['date'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USERS\n",
    "df_users_day_stats = pd.read_pickle(dir_path + '__users_day_stats.pickle')\n",
    "df_users_day_stats = df_users_day_stats.reset_index()\n",
    "df_users_day_stats = add_temporal_information(df_users_day_stats,col='createdAt')\n",
    "df_users_day_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = df_users_day_stats.groupby('userId')[['tweetId']].sum()\n",
    "aa.reset_index().to_pickle(dir_path + '__tweets_user_all.pickle')\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = df_users_day_stats.groupby(['userId','week_of_year'])[['tweetId']].sum()\n",
    "bb.reset_index().to_pickle(dir_path + '__tweets_user_week.pickle')\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = df_users_day_stats.groupby(['userId','month'])[['tweetId']].sum()\n",
    "cc.reset_index().to_pickle(dir_path + '__tweets_user_month.pickle')\n",
    "cc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
